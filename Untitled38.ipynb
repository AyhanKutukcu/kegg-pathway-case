{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1daefacd-760a-4aae-a4e0-c1feef706cc9",
   "metadata": {},
   "source": [
    "# Verilen Veri Setinini İçeriği"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e37521c2-12b1-4abf-9839-d0088411da4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       converted_alias    mock_rep1    mock_rep2    mock_rep3  sars_cov_rep1  \\\n",
      "0      ENSG00000290825      0.03958      0.08353      0.08404        0.08911   \n",
      "1      ENSG00000278267      1.92300      3.04400      2.72200        2.16500   \n",
      "2      ENSG00000284332      0.00000      0.00000      0.00000        0.00000   \n",
      "3      ENSG00000237613      0.00000      0.00000      0.00000        0.00000   \n",
      "4      ENSG00000186092      0.00000      0.00000      0.00000        0.00000   \n",
      "...                ...          ...          ...          ...            ...   \n",
      "26114  ENSG00000212907   6446.00000   9613.00000  11580.00000     6564.00000   \n",
      "26115  ENSG00000198886  15420.00000  15630.00000  13400.00000    14620.00000   \n",
      "26116  ENSG00000198786   8609.00000   9019.00000   7286.00000     7628.00000   \n",
      "26117  ENSG00000198695  12270.00000  13240.00000  10410.00000    10550.00000   \n",
      "26118  ENSG00000198727   7928.00000   7369.00000   6012.00000     7487.00000   \n",
      "\n",
      "       sars_cov_rep2  sars_cov_rep3  \n",
      "0            0.08877        0.06094  \n",
      "1            2.15600        3.70100  \n",
      "2            0.00000        0.18240  \n",
      "3            0.00000        0.00000  \n",
      "4            0.00000        0.00000  \n",
      "...              ...            ...  \n",
      "26114     6939.00000     7220.00000  \n",
      "26115    16650.00000    15730.00000  \n",
      "26116     9205.00000     8384.00000  \n",
      "26117    12490.00000    11460.00000  \n",
      "26118     8709.00000     8027.00000  \n",
      "\n",
      "[26119 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TSV dosyasının yolunu belirtin\n",
    "dosya_yolu = \"C:/Users/ayhan/Downloads/assesment_dataset_converter/assesment_dataset.tsv\"\n",
    "\n",
    "# TSV dosyasını oku\n",
    "df = pd.read_csv(dosya_yolu, sep='\\t')\n",
    "\n",
    "# Dosyanın içeriğini göster\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2e76d9-6824-4a81-9eaf-f70ab7cec26a",
   "metadata": {},
   "source": [
    "# Her Durumda İfade İçermeyen Genlerin Veri Setinden Çıkarılması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d85e6850-0350-44fc-92f7-c30d84c0f9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       converted_alias    mock_rep1    mock_rep2    mock_rep3  sars_cov_rep1  \\\n",
      "0      ENSG00000290825      0.03958      0.08353      0.08404        0.08911   \n",
      "1      ENSG00000278267      1.92300      3.04400      2.72200        2.16500   \n",
      "2      ENSG00000284332      0.00000      0.00000      0.00000        0.00000   \n",
      "5      ENSG00000290825      0.12210      0.04294      0.05760        0.04580   \n",
      "6      ENSG00000273874      0.96150      2.02900      0.68060        1.08200   \n",
      "...                ...          ...          ...          ...            ...   \n",
      "26114  ENSG00000212907   6446.00000   9613.00000  11580.00000     6564.00000   \n",
      "26115  ENSG00000198886  15420.00000  15630.00000  13400.00000    14620.00000   \n",
      "26116  ENSG00000198786   8609.00000   9019.00000   7286.00000     7628.00000   \n",
      "26117  ENSG00000198695  12270.00000  13240.00000  10410.00000    10550.00000   \n",
      "26118  ENSG00000198727   7928.00000   7369.00000   6012.00000     7487.00000   \n",
      "\n",
      "       sars_cov_rep2  sars_cov_rep3  \n",
      "0            0.08877        0.06094  \n",
      "1            2.15600        3.70100  \n",
      "2            0.00000        0.18240  \n",
      "5            0.13690        0.12530  \n",
      "6            1.07800        1.11000  \n",
      "...              ...            ...  \n",
      "26114     6939.00000     7220.00000  \n",
      "26115    16650.00000    15730.00000  \n",
      "26116     9205.00000     8384.00000  \n",
      "26117    12490.00000    11460.00000  \n",
      "26118     8709.00000     8027.00000  \n",
      "\n",
      "[20048 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Orijinal TSV dosyasının yolunu belirtin\n",
    "dosya_yolu = 'C:/Users/ayhan/Downloads/assesment_dataset_converter/assesment_dataset.tsv'\n",
    "\n",
    "# Orijinal dosyanın dizin yolunu al\n",
    "dizin_yolu = os.path.dirname(dosya_yolu)\n",
    "\n",
    "# Yeni dosya yolunu oluştur\n",
    "kopya_dosya_yolu = os.path.join(dizin_yolu, 'temizlenmis_dosya.tsv')\n",
    "\n",
    "# TSV dosyasını oku\n",
    "df = pd.read_csv(dosya_yolu, sep='\\t')\n",
    "\n",
    "# Sadece \"mock_rep1\", \"mock_rep2\", \"mock_rep3\", \"sars_cov_rep1\", \"sars_cov_rep2\", \"sars_cov_rep3\" sütunlarını içeren satırların hepsi 0 olan satırları çıkar\n",
    "sütunlar = [\"mock_rep1\", \"mock_rep2\", \"mock_rep3\", \"sars_cov_rep1\", \"sars_cov_rep2\", \"sars_cov_rep3\"]\n",
    "df = df[(df[sütunlar] != 0).any(axis=1)]\n",
    "\n",
    "# Temizlenmiş veri setini yeni bir TSV dosyasına kaydet\n",
    "df.to_csv(kopya_dosya_yolu, sep='\\t', index=False)\n",
    "\n",
    "# Temizlenmiş veri setini yazdır\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2006f4b-916b-4cb2-a069-1bace2fb1532",
   "metadata": {},
   "source": [
    "# Anlamlı Olarak Değişen Genler İçin Örnek Hesaplanması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e2b2aba-df44-4d25-921a-5b203af46efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gene ENSG00000290825 için mock ve SARS-CoV grupları arasında anlamlı farklılık yoktur (p-value = 0.5776996762823065).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Veri çerçevesini oluşturalım (örnek veriye dayanarak)\n",
    "data = {\n",
    "    'converted_alias': ['ENSG00000290825', 'ENSG00000278267', 'ENSG00000284332'],\n",
    "    'mock_rep1': [0.03958, 1.923, 0.0],\n",
    "    'mock_rep2': [0.08353, 3.044, 0.0],\n",
    "    'mock_rep3': [0.08404, 2.722, 0.0],\n",
    "    'sars_cov_rep1': [0.08911, 2.165, 0.0],\n",
    "    'sars_cov_rep2': [0.08877, 2.156, 0.0],\n",
    "    'sars_cov_rep3': [0.06094, 3.701, 0.1824]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Mock ve SARS-CoV grupları arasındaki farklı ifadeleri belirlemek için t-testi yapalım\n",
    "alpha = 0.05\n",
    "\n",
    "# Örnek olarak ilk gen için t-testi yapalım (ENSG00000290825)\n",
    "gene = 'ENSG00000290825'\n",
    "mock_expr = df.loc[df['converted_alias'] == gene, ['mock_rep1', 'mock_rep2', 'mock_rep3']].values.flatten()\n",
    "sars_cov_expr = df.loc[df['converted_alias'] == gene, ['sars_cov_rep1', 'sars_cov_rep2', 'sars_cov_rep3']].values.flatten()\n",
    "\n",
    "t_stat, p_val = ttest_ind(mock_expr, sars_cov_expr)\n",
    "\n",
    "# P-value kontrolü\n",
    "if p_val < alpha:\n",
    "    print(f\"Gene {gene} için mock ve SARS-CoV grupları arasında anlamlı farklılık vardır (p-value = {p_val}).\")\n",
    "else:\n",
    "    print(f\"Gene {gene} için mock ve SARS-CoV grupları arasında anlamlı farklılık yoktur (p-value = {p_val}).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab1c661-6df3-4a94-9dfb-eaec728213dd",
   "metadata": {},
   "source": [
    "# Tüm Listede Hesaplama Yapılarak Sadece Anlamlı Genlerin Bırakılması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57531910-f794-4ae6-bbcc-c5f0cb808e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anlamlı genler içeren dosya kaydedildi: C:\\Users\\ayhan\\Downloads\\assesment_dataset_converter\\anlamli_genler.tsv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "import numpy as np\n",
    "\n",
    "# Dosyayı oku\n",
    "file_path = r\"C:\\Users\\ayhan\\Downloads\\assesment_dataset_converter\\temizlenmis_dosya.tsv\"\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "# P-value eşiği\n",
    "alpha = 0.05\n",
    "\n",
    "# Anlamlı farklılıkları tutacak bir liste oluşturalım\n",
    "significant_genes = []\n",
    "\n",
    "# Her bir gen için differential expression analizini yapalım\n",
    "for index, row in df.iterrows():\n",
    "    gene = row['converted_alias']\n",
    "    mock_expr = pd.to_numeric(row[['mock_rep1', 'mock_rep2', 'mock_rep3']], errors='coerce').dropna()\n",
    "    sars_cov_expr = pd.to_numeric(row[['sars_cov_rep1', 'sars_cov_rep2', 'sars_cov_rep3']], errors='coerce').dropna()\n",
    "    \n",
    "    t_stat, p_val = ttest_ind(mock_expr, sars_cov_expr)\n",
    "    \n",
    "    # P-value kontrolü\n",
    "    if p_val < alpha:\n",
    "        significant_genes.append(row)  # Anlamlı olan genin satırını listeye ekle\n",
    "    \n",
    "# Anlamlı genleri içeren yeni veri çerçevesini oluştur\n",
    "significant_df = pd.DataFrame(significant_genes)\n",
    "\n",
    "# Yeni dosyayı kaydet\n",
    "output_file_path = r\"C:\\Users\\ayhan\\Downloads\\assesment_dataset_converter\\anlamli_genler.tsv\"\n",
    "significant_df.to_csv(output_file_path, sep='\\t', index=False)\n",
    "\n",
    "print(f\"Anlamlı genler içeren dosya kaydedildi: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f85a6f-d66f-434f-a6fd-e6ef6fe2e086",
   "metadata": {},
   "source": [
    "# Anlamlı Genler Listesinin İçeriğinin Görüntülenmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "64cf0b49-5e4e-4fb0-954f-38a0f91aad66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      converted_alias  mock_rep1  mock_rep2  mock_rep3  sars_cov_rep1  \\\n",
      "0     ENSG00000284662    0.01131    0.01194   0.008007        0.00000   \n",
      "1     ENSG00000291156    0.67130    0.49590   0.617700        1.20900   \n",
      "2     ENSG00000248333   16.24000   15.21000  15.690000       16.53000   \n",
      "3     ENSG00000169598    0.94210    0.79060   0.916000        1.11200   \n",
      "4     ENSG00000236423    0.10050    0.17670   0.118500        0.00000   \n",
      "...               ...        ...        ...        ...            ...   \n",
      "2066  ENSG00000183837    0.00000    0.00000   0.000000        0.01999   \n",
      "2067  ENSG00000102030   29.00000   28.37000  26.190000       34.02000   \n",
      "2068  ENSG00000207165   10.65000   10.73000   9.942000        9.81400   \n",
      "2069  ENSG00000071859   56.87000   59.30000  58.670000       71.24000   \n",
      "2070  ENSG00000182712    3.85900    3.91600   4.097000        3.42500   \n",
      "\n",
      "      sars_cov_rep2  sars_cov_rep3  \n",
      "0           0.00000       0.004354  \n",
      "1           0.97860       1.447000  \n",
      "2          17.31000      17.890000  \n",
      "3           1.14600       1.088000  \n",
      "4           0.07512       0.025790  \n",
      "...             ...            ...  \n",
      "2066        0.01991       0.034180  \n",
      "2067       30.51000      33.280000  \n",
      "2068        9.23300       9.321000  \n",
      "2069       66.68000      65.600000  \n",
      "2070        2.91300       3.485000  \n",
      "\n",
      "[2071 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TSV dosyasının yolunu belirtin\n",
    "dosya_yolu = \"C:/Users/ayhan/Downloads/assesment_dataset_converter/anlamli_genler.tsv\"\n",
    "\n",
    "# TSV dosyasını oku\n",
    "df = pd.read_csv(dosya_yolu, sep='\\t')\n",
    "\n",
    "# Dosyanın içeriğini göster\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2049336e-a654-47cc-b61f-817a439b72b2",
   "metadata": {},
   "source": [
    "# Verilen Converter Dosyası İçerisindeki ENTREZ ID'lerin ENSEMBL ID'ler İle Uyuşmasının Kontrol Edilmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9f15d2a2-1423-42ef-aaa8-1f18cd2f862c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n",
      "<!DOCTYPE eSearchResult PUBLIC \"-//NLM//DTD esearch 20060628//EN\" \"https://eutils.ncbi.nlm.nih.gov/eutils/dtd/20060628/esearch.dtd\">\n",
      "<eSearchResult><Count>1</Count><RetMax>1</RetMax><RetStart>0</RetStart><IdList>\n",
      "<Id>100302278</Id>\n",
      "</IdList><TranslationSet/><TranslationStack>   <TermSet>    <Term>ENSG00000284332[All Fields]</Term>    <Field>All Fields</Field>    <Count>1</Count>    <Explode>N</Explode>   </TermSet>   <OP>GROUP</OP>  </TranslationStack><QueryTranslation>ENSG00000284332[All Fields]</QueryTranslation><ErrorList><FieldNotFound>Source ID</FieldNotFound></ErrorList></eSearchResult>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def get_entrez_id(ensembl_id):\n",
    "    url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=gene&term={ensembl_id}[Source+ID]\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "ensembl_id = \"ENSG00000284332\"\n",
    "entrez_id = get_entrez_id(ensembl_id)\n",
    "print(entrez_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e70e3e-b154-48b0-8ee5-210d03aa8e9e",
   "metadata": {},
   "source": [
    "# ENSEMBL ID'lerin ENTREZ ID'ler İle Değiştirilmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "559cc1ef-78be-4cae-a8ac-5a203f9f67de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtrelenmiş dosya C:/Users/ayhan/Downloads/assesment_dataset_converter/new_tsv.csv olarak başarıyla kaydedildi.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TSV dosyasının yolunu belirtin\n",
    "input_file = \"C:/Users/ayhan/Downloads/assesment_dataset_converter/converter.tsv\"\n",
    "temp_tsv_file = \"C:/Users/ayhan/Downloads/assesment_dataset_converter/temp_tsv_file.tsv\"\n",
    "csv_file = \"C:/Users/ayhan/Downloads/assesment_dataset_converter/new_tsv.csv\"\n",
    "\n",
    "\n",
    "# TSV dosyasını oku\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, open(temp_tsv_file, 'w', encoding='utf-8') as outfile:\n",
    "    for line in infile:\n",
    "        \n",
    "        # Satırı virgüle göre ayır\n",
    "        if \"None\" not in line:\n",
    "            columns = line.strip().split(',')\n",
    "            # \"None\" içeren satırları atla\n",
    "            # TSV formatında yeni dosyaya yaz\n",
    "            tsv_line = '\\t'.join(columns)\n",
    "            outfile.write(tsv_line + '\\n')\n",
    "        \n",
    "df = pd.read_csv(temp_tsv_file, sep='\\t', header=None)\n",
    "\n",
    "# Yeni CSV dosyasının yolunu belirtin\n",
    "\n",
    "# Filtrelenmiş DataFrame'i CSV dosyası olarak kaydet\n",
    "df.to_csv(csv_file, index=False, header=False)\n",
    "\n",
    "print(f\"Filtrelenmiş dosya {csv_file} olarak başarıyla kaydedildi.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19fabf21-40c1-446c-aa1a-498fbf94a956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Güncellenmiş dosya C:/Users/ayhan/Downloads/assesment_dataset_converter/yeni_anlamli_genler.tsv olarak başarıyla kaydedildi.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dosya yollarını belirtin\n",
    "csv_file = \"C:/Users/ayhan/Downloads/assesment_dataset_converter/new_tsv.csv\"\n",
    "anlamli_genler_file = \"C:/Users/ayhan/Downloads/assesment_dataset_converter/anlamli_genler.tsv\"\n",
    "yeni_anlamli_genler = \"C:/Users/ayhan/Downloads/assesment_dataset_converter/yeni_anlamli_genler.tsv\"\n",
    "\n",
    "# TSV dosyalarını oku\n",
    "anlamli_genler_df = pd.read_csv(anlamli_genler_file, sep='\\t')\n",
    "new_tsv_df = pd.read_csv(csv_file, header=None, names=[\"initial_alias\", \"converted_alias\", \"name\", \"description\", \"namespace\"])\n",
    "\n",
    "# initial_alias değerleri ile anlamli_genler_df'deki converted_alias değerlerini karşılaştır\n",
    "for index, row in new_tsv_df.iterrows():\n",
    "    match_index = anlamli_genler_df[anlamli_genler_df['converted_alias'] == row['converted_alias']].index\n",
    "    if not match_index.empty:\n",
    "        anlamli_genler_df.at[match_index[0], 'converted_alias'] = row['initial_alias']\n",
    "\n",
    "# Güncellenmiş DataFrame'i yeni TSV dosyasına yaz\n",
    "anlamli_genler_df.to_csv(yeni_anlamli_genler, sep='\\t', index=False)\n",
    "\n",
    "print(f\"Güncellenmiş dosya {yeni_anlamli_genler} olarak başarıyla kaydedildi.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0acf8301-a228-41cd-8270-05394298dd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Güncellenmiş dosya C:/Users/ayhan/Downloads/assesment_dataset_converter/yeni_anlamli_genler.tsv olarak başarıyla kaydedildi.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dosya yolunu belirtin\n",
    "yeni_anlamli_genler_file = \"C:/Users/ayhan/Downloads/assesment_dataset_converter/yeni_anlamli_genler.tsv\"\n",
    "\n",
    "# TSV dosyasını oku\n",
    "yeni_anlamli_genler_df = pd.read_csv(yeni_anlamli_genler_file, sep='\\t')\n",
    "\n",
    "# 'converted_alias' sütununda 'ENSG' içeren satırları sil\n",
    "filtered_df = yeni_anlamli_genler_df[~yeni_anlamli_genler_df['converted_alias'].str.contains('ENSG', na=False)]\n",
    "\n",
    "# Güncellenmiş DataFrame'i tekrar TSV dosyasına yaz\n",
    "filtered_df.to_csv(yeni_anlamli_genler_file, sep='\\t', index=False)\n",
    "\n",
    "print(f\"Güncellenmiş dosya {yeni_anlamli_genler_file} olarak başarıyla kaydedildi.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b9b410-1452-4fb3-81c0-1cfdd78add78",
   "metadata": {},
   "source": [
    "# Anlamlı Genlerin Fold Change'lerinin Hesaplanması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd345ba3-076d-4772-beb6-a8f59ef99c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold change hesaplandı ve C:/Users/ayhan/Downloads/assesment_dataset_converter/fold_change_yeni_anlamli_genler.tsv dosyasına kaydedildi.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dosyayı oku\n",
    "file_path = \"C:/Users/ayhan/Downloads/assesment_dataset_converter/yeni_anlamli_genler.tsv\"\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "# Fold change hesapla\n",
    "for i in range(1, 4):  # Toplam 3 mock ve 3 sars_cov sütunu var\n",
    "    mock_col = f'mock_rep{i}'\n",
    "    sars_cov_col = f'sars_cov_rep{i}'\n",
    "    fold_change_col = f'fold_change_rep{i}'\n",
    "    \n",
    "    df[fold_change_col] = (df[sars_cov_col] + 1e-6) / (df[mock_col] + 1e-6)  # Sıfıra bölme hatasını önlemek için küçük bir sayı ekliyoruz\n",
    "\n",
    "# Hesaplanan fold change'leri içeren dosyayı kaydet\n",
    "output_file = 'C:/Users/ayhan/Downloads/assesment_dataset_converter/fold_change_yeni_anlamli_genler.tsv'\n",
    "df.to_csv(output_file, sep='\\t', index=False)\n",
    "\n",
    "print(f'Fold change hesaplandı ve {output_file} dosyasına kaydedildi.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "233ce134-20b2-4b8b-a9f9-6213c61c6ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Her satır için fold change değerleri hesaplandı ve C:/Users/ayhan/Downloads/assesment_dataset_converter/fold_change_her_satir_icin.tsv dosyasına kaydedildi.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dosyayı oku\n",
    "file_path = \"C:/Users/ayhan/Downloads/assesment_dataset_converter/yeni_anlamli_genler.tsv\"\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "# Her satır için fold change hesapla\n",
    "for index, row in df.iterrows():\n",
    "    mock_mean = row[['mock_rep1', 'mock_rep2', 'mock_rep3']].mean()\n",
    "    sars_cov_mean = row[['sars_cov_rep1', 'sars_cov_rep2', 'sars_cov_rep3']].mean()\n",
    "    \n",
    "    fold_change = (sars_cov_mean + 1e-6) / (mock_mean + 1e-6)  # Sıfıra bölme hatasını önlemek için küçük bir sayı ekliyoruz\n",
    "    \n",
    "    # Hesaplanan fold change değerini ilgili satıra yaz\n",
    "    df.at[index, 'fold_change'] = fold_change\n",
    "\n",
    "# Hesaplanan fold change'leri içeren dosyayı kaydet\n",
    "output_file = 'C:/Users/ayhan/Downloads/assesment_dataset_converter/fold_change_her_satir_icin.tsv'\n",
    "df.to_csv(output_file, sep='\\t', index=False)\n",
    "\n",
    "print(f'Her satır için fold change değerleri hesaplandı ve {output_file} dosyasına kaydedildi.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9850f481-6124-4d03-b8b6-036f65bb52dd",
   "metadata": {},
   "source": [
    "# GProfiler'dan Alınan Çıktıların Sinyal Yollarının Listelenmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73809020-bca6-4586-aee1-3b92b6287a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    source                                          term_name     term_id  \\\n",
      "100  GO:BP            cell surface receptor signaling pathway  GO:0007166   \n",
      "122  GO:BP                        apoptotic signaling pathway  GO:0097190   \n",
      "155  GO:BP                cytokine-mediated signaling pathway  GO:0019221   \n",
      "168  GO:BP              extrinsic apoptotic signaling pathway  GO:0097191   \n",
      "292  GO:BP          regulation of apoptotic signaling pathway  GO:2001233   \n",
      "310  GO:BP              intrinsic apoptotic signaling pathway  GO:0097193   \n",
      "416  GO:BP  regulation of extrinsic apoptotic signaling pa...  GO:2001236   \n",
      "438  GO:BP   enzyme-linked receptor protein signaling pathway  GO:0007167   \n",
      "452  GO:BP                              Wnt signaling pathway  GO:0016055   \n",
      "524   KEGG                              TNF signaling pathway  KEGG:04668   \n",
      "526   KEGG                            IL-17 signaling pathway  KEGG:04657   \n",
      "527   KEGG                       NF-kappa B signaling pathway  KEGG:04064   \n",
      "551     WP        Network map of SARS CoV 2 signaling pathway   WP:WP5115   \n",
      "\n",
      "     highlighted  adjusted_p_value  negative_log10_of_adjusted_p_value  \\\n",
      "100        False      5.276017e-11                           10.277694   \n",
      "122        False      1.795484e-09                            8.745818   \n",
      "155        False      1.305929e-07                            6.884081   \n",
      "168        False      3.504567e-07                            6.455366   \n",
      "292        False      5.721278e-04                            3.242507   \n",
      "310        False      1.337921e-03                            2.873570   \n",
      "416        False      2.275177e-02                            1.642985   \n",
      "438        False      3.056658e-02                            1.514753   \n",
      "452        False      4.022774e-02                            1.395474   \n",
      "524        False      8.900891e-07                            6.050566   \n",
      "526        False      5.534697e-05                            4.256906   \n",
      "527        False      5.062964e-04                            3.295595   \n",
      "551        False      9.914506e-04                            3.003729   \n",
      "\n",
      "     term_size  query_size  intersection_size  effective_domain_size  \\\n",
      "100       2799        1749                341                  21031   \n",
      "122        607        1749                104                  21031   \n",
      "155        499        1749                 86                  21031   \n",
      "168        228        1749                 50                  21031   \n",
      "292        382        1749                 62                  21031   \n",
      "310        315        1749                 53                  21031   \n",
      "416        155        1749                 30                  21031   \n",
      "438        967        1749                118                  21031   \n",
      "452        452        1749                 64                  21031   \n",
      "524        113         892                 34                   8484   \n",
      "526         92         892                 27                   8484   \n",
      "527        102         892                 27                   8484   \n",
      "551        217         921                 47                   8286   \n",
      "\n",
      "                                         intersections  \n",
      "100  ENSG00000131697,ENSG00000116285,ENSG0000017160...  \n",
      "122  ENSG00000130939,ENSG00000126709,ENSG0000011676...  \n",
      "155  ENSG00000117115,ENSG00000117525,ENSG0000016071...  \n",
      "168  ENSG00000126709,ENSG00000023909,ENSG0000013424...  \n",
      "292  ENSG00000126709,ENSG00000116761,ENSG0000002390...  \n",
      "310  ENSG00000126709,ENSG00000213190,ENSG0000016322...  \n",
      "416  ENSG00000126709,ENSG00000023909,ENSG0000011658...  \n",
      "438  ENSG00000116285,ENSG00000171608,ENSG0000012670...  \n",
      "452  ENSG00000131697,ENSG00000178585,ENSG0000016240...  \n",
      "524  ENSG00000171608,ENSG00000073756,ENSG0000012553...  \n",
      "526  ENSG00000163220,ENSG00000143546,ENSG0000018433...  \n",
      "527  ENSG00000073756,ENSG00000125538,ENSG0000016942...  \n",
      "551  ENSG00000126709,ENSG00000137959,ENSG0000007375...  \n",
      "\"signal pathway\" kelimesini içeren satırlar C:/Users/ayhan/Downloads/assesment_dataset_converter/signal_path.csv dosyasına kaydedildi.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dosyayı oku\n",
    "file_path = \"C:/Users/ayhan/Downloads/assesment_dataset_converter/gProfiler_hsapiens_09.07.2024 21-03-30__intersections.csv\"\n",
    "df = pd.read_csv(file_path, sep=',')\n",
    "\n",
    "# \"signal pathway\" kelimesini içeren satırları filtrele\n",
    "filtered_df = df[df[\"term_name\"].str.contains(\"signaling pathway\", case=False, na=False)]\n",
    "print(filtered_df)\n",
    "\n",
    "# Filtrelenmiş veriyi signal_path.csv dosyasına kaydet\n",
    "output_file = \"C:/Users/ayhan/Downloads/assesment_dataset_converter/signal_path.csv\"\n",
    "filtered_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f'\"signal pathway\" kelimesini içeren satırlar {output_file} dosyasına kaydedildi.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67463a98-d6f3-4b0d-8827-84b759e3b942",
   "metadata": {},
   "source": [
    "# Sinyal Yollarına Entrez ID'lerin Eklenmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "938e2ff8-beba-4c68-a92e-98868ec3f3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"intersections\" sütunu güncellendi ve C:/Users/ayhan/Downloads/assesment_dataset_converter/signal_path.csv dosyasına kaydedildi.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dosyaları oku\n",
    "signal_path_file = \"C:/Users/ayhan/Downloads/assesment_dataset_converter/signal_path.csv\"\n",
    "tmp_tsv_file = \"C:/Users/ayhan/Downloads/assesment_dataset_converter/temp_tsv_file.tsv\"\n",
    "\n",
    "signal_path_df = pd.read_csv(signal_path_file)\n",
    "tmp_tsv_df = pd.read_csv(tmp_tsv_file, sep='\\t')\n",
    "\n",
    "# 'intersections' sütununu array'e al\n",
    "array_intersec = signal_path_df[\"intersections\"]\n",
    "\n",
    "# Güncellenmiş 'intersections' verilerini saklamak için bir liste oluştur\n",
    "updated_intersections = []\n",
    "\n",
    "for i in array_intersec:\n",
    "    array_tmp = []\n",
    "    tmp_str = \"\"\n",
    "    for j in i:\n",
    "        if j != ',':\n",
    "            tmp_str += j\n",
    "        else:\n",
    "            array_tmp.append(tmp_str.strip())\n",
    "            tmp_str = \"\"\n",
    "    array_tmp.append(tmp_str.strip())  # son değeri de ekle\n",
    "\n",
    "    # Değerleri karşılaştır ve eşleşenleri değiştir\n",
    "    updated_array = []\n",
    "    for item in array_tmp:\n",
    "        match = tmp_tsv_df[tmp_tsv_df['converted_alias'] == item]\n",
    "        if not match.empty:\n",
    "            updated_array.append(match['initial_alias'].values[0])\n",
    "        else:\n",
    "            updated_array.append(item)\n",
    "    \n",
    "    # Güncellenmiş değeri listeye ekle\n",
    "    updated_intersections.append(','.join(map(str, updated_array)))\n",
    "\n",
    "# Güncellenmiş 'intersections' sütununu DataFrame'e yaz\n",
    "signal_path_df['intersections'] = updated_intersections\n",
    "\n",
    "# Güncellenmiş DataFrame'i signal_path.csv dosyasına yaz\n",
    "signal_path_df.to_csv(signal_path_file, index=False)\n",
    "\n",
    "print(f'\"intersections\" sütunu güncellendi ve {signal_path_file} dosyasına kaydedildi.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8e06db-0848-45ae-b689-47a1a65aa045",
   "metadata": {},
   "source": [
    "# KEGG PATHWAY İçin Yükleme Yaparken Genlerin Belli Olabilmesi Adına Genlerin Yanına Renk Eklenmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f30a168c-9fa9-4576-ab9a-7127980c21f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#4e4e4e\n",
      "#4e4e4e\n"
     ]
    }
   ],
   "source": [
    "# RGB hex renk kodlarının bulunduğu liste\n",
    "hex_colors = [\n",
    "    \"#000000\", \"#800000\", \"#008000\", \"#808000\", \"#000080\",\n",
    "    \"#800080\", \"#008080\", \"#c0c0c0\", \"#808080\", \"#ff0000\",\n",
    "    \"#00ff00\", \"#ffff00\", \"#0000ff\", \"#ff00ff\", \"#00ffff\",\n",
    "    \"#ffffff\", \"#000000\", \"#00005f\", \"#000087\", \"#0000af\",\n",
    "    \"#0000d7\", \"#0000ff\", \"#005f00\", \"#005f5f\", \"#005f87\",\n",
    "    \"#005faf\", \"#005fd7\", \"#005fff\", \"#008700\", \"#00875f\",\n",
    "    \"#008787\", \"#0087af\", \"#0087d7\", \"#0087ff\", \"#00af00\",\n",
    "    \"#00af5f\", \"#00af87\", \"#00afaf\", \"#00afd7\", \"#00afff\",\n",
    "    \"#00d700\", \"#00d75f\", \"#00d787\", \"#00d7af\", \"#00d7d7\",\n",
    "    \"#00d7ff\", \"#00ff00\", \"#00ff5f\", \"#00ff87\", \"#00ffaf\",\n",
    "    \"#00ffd7\", \"#00ffff\", \"#5f0000\", \"#5f005f\", \"#5f0087\",\n",
    "    \"#5f00af\", \"#5f00d7\", \"#5f00ff\", \"#5f5f00\", \"#5f5f5f\",\n",
    "    \"#5f5f87\", \"#5f5faf\", \"#5f5fd7\", \"#5f5fff\", \"#5f8700\",\n",
    "    \"#5f875f\", \"#5f8787\", \"#5f87af\", \"#5f87d7\", \"#5f87ff\",\n",
    "    \"#5faf00\", \"#5faf5f\", \"#5faf87\", \"#5fafaf\", \"#5fafd7\",\n",
    "    \"#5fafff\", \"#5fd700\", \"#5fd75f\", \"#5fd787\", \"#5fd7af\",\n",
    "    \"#5fd7d7\", \"#5fd7ff\", \"#5fff00\", \"#5fff5f\", \"#5fff87\",\n",
    "    \"#5fffaf\", \"#5fffd7\", \"#5fffff\", \"#870000\", \"#87005f\",\n",
    "    \"#870087\", \"#8700af\", \"#8700d7\", \"#8700ff\", \"#875f00\",\n",
    "    \"#875f5f\", \"#875f87\", \"#875faf\", \"#875fd7\", \"#875fff\",\n",
    "    \"#878700\", \"#87875f\", \"#878787\", \"#8787af\", \"#8787d7\",\n",
    "    \"#8787ff\", \"#87af00\", \"#87af5f\", \"#87af87\", \"#87afaf\",\n",
    "    \"#87afd7\", \"#87afff\", \"#87d700\", \"#87d75f\", \"#87d787\",\n",
    "    \"#87d7af\", \"#87d7d7\", \"#87d7ff\", \"#87ff00\", \"#87ff5f\",\n",
    "    \"#87ff87\", \"#87ffaf\", \"#87ffd7\", \"#87ffff\", \"#af0000\",\n",
    "    \"#af005f\", \"#af0087\", \"#af00af\", \"#af00d7\", \"#af00ff\",\n",
    "    \"#af5f00\", \"#af5f5f\", \"#af5f87\", \"#af5faf\", \"#af5fd7\",\n",
    "    \"#af5fff\", \"#af8700\", \"#af875f\", \"#af8787\", \"#af87af\",\n",
    "    \"#af87d7\", \"#af87ff\", \"#afaf00\", \"#afaf5f\", \"#afaf87\",\n",
    "    \"#afafaf\", \"#afafd7\", \"#afafff\", \"#afd700\", \"#afd75f\",\n",
    "    \"#afd787\", \"#afd7af\", \"#afd7d7\", \"#afd7ff\", \"#afff00\",\n",
    "    \"#afff5f\", \"#afff87\", \"#afffaf\", \"#afffd7\", \"#afffff\",\n",
    "    \"#d70000\", \"#d7005f\", \"#d70087\", \"#d700af\", \"#d700d7\",\n",
    "    \"#d700ff\", \"#d75f00\", \"#d75f5f\", \"#d75f87\", \"#d75faf\",\n",
    "    \"#d75fd7\", \"#d75fff\", \"#d78700\", \"#d7875f\", \"#d78787\",\n",
    "    \"#d787af\", \"#d787d7\", \"#d787ff\", \"#d7af00\", \"#d7af5f\",\n",
    "    \"#d7af87\", \"#d7afaf\", \"#d7afd7\", \"#d7afff\", \"#d7d700\",\n",
    "    \"#d7d75f\", \"#d7d787\", \"#d7d7af\", \"#d7d7d7\", \"#d7d7ff\",\n",
    "    \"#d7ff00\", \"#d7ff5f\", \"#d7ff87\", \"#d7ffaf\", \"#d7ffd7\",\n",
    "    \"#d7ffff\", \"#ff0000\", \"#ff005f\", \"#ff0087\", \"#ff00af\",\n",
    "    \"#ff00d7\", \"#ff00ff\", \"#ff5f00\", \"#ff5f5f\", \"#ff5f87\",\n",
    "    \"#ff5faf\", \"#ff5fd7\", \"#ff5fff\", \"#ff8700\", \"#ff875f\",\n",
    "    \"#ff8787\", \"#ff87af\", \"#ff87d7\", \"#ff87ff\", \"#ffaf00\",\n",
    "    \"#ffaf5f\", \"#ffaf87\", \"#ffafaf\", \"#ffafd7\", \"#ffafff\",\n",
    "    \"#ffd700\", \"#ffd75f\", \"#ffd787\", \"#ffd7af\", \"#ffd7d7\",\n",
    "    \"#ffd7ff\", \"#ffff00\", \"#ffff5f\", \"#ffff87\", \"#ffffaf\",\n",
    "    \"#ffffd7\", \"#ffffff\", \"#080808\", \"#121212\", \"#1c1c1c\",\n",
    "    \"#262626\", \"#303030\", \"#3a3a3a\", \"#444444\", \"#4e4e4e\",\n",
    "    \"#585858\", \"#606060\", \"#666666\", \"#767676\", \"#808080\",\n",
    "    \"#8a8a8a\", \"#949494\", \"#9e9e9e\", \"#a8a8a8\", \"#b2b2b2\",\n",
    "    \"#bcbcbc\", \"#c6c6c6\", \"#d0d0d0\", \"#dadada\", \"#e4e4e4\",\n",
    "    \"#eeeeee\"\n",
    "]\n",
    "\n",
    "# Rastgele bir renk kodu seçen fonksiyon\n",
    "def pick_random_color(hex_colors, seed=None):\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    return random.choice(hex_colors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e69a37fb-f767-4696-88f9-b6a6a3d9b547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GO:0007166\n",
      "GO:0097190\n",
      "GO:0019221\n",
      "GO:0097191\n",
      "GO:2001233\n",
      "GO:0097193\n",
      "GO:2001236\n",
      "GO:0007167\n",
      "GO:0016055\n",
      "KEGG:04668\n",
      "KEGG:04657\n",
      "KEGG:04064\n",
      "REAC:R-HSA-918233\n",
      "WP:WP5115\n",
      "WP:WP2882\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dosyaları oku\n",
    "signal_path_file = \"C:/Users/ayhan/Downloads/assesment_dataset_converter/signal_path.csv\"\n",
    "fold_change_file = \"C:/Users/ayhan/Downloads/assesment_dataset_converter/fold_change_her_satir_icin.tsv\"\n",
    "output_file = \"C:/Users/ayhan/Downloads/assesment_dataset_converter/new_last.csv\"\n",
    "\n",
    "signal_path_df = pd.read_csv(signal_path_file)\n",
    "fold_change_df = pd.read_csv(fold_change_file, sep='\\t')\n",
    "\n",
    "array_intersec = signal_path_df[\"intersections\"]\n",
    "array_term_ids = signal_path_df[\"term_id\"]\n",
    "\n",
    "count = 0\n",
    "\n",
    "directory_path = \"C:/Users/ayhan/Downloads/assesment_dataset_converter/sinyal\"\n",
    "fold_change_file_path = \"C:/Users/ayhan/Downloads/assesment_dataset_converter/fold_change_her_satir_icin.tsv\"\n",
    "\n",
    "fold_change_df = pd.read_csv(fold_change_file_path, sep=\"\\t\")\n",
    "#print(fold_change_df[\"converted_alias\"])\n",
    "#print(fold_change_df[\"fold_change\"])\n",
    "\n",
    "converted_alias_list = fold_change_df[\"converted_alias\"].tolist()\n",
    "index_of_value = converted_alias_list.index(100133612)\n",
    "\n",
    "while count < len(array_term_ids):  \n",
    "    print(array_term_ids[count])\n",
    "    str_name = array_term_ids[count]\n",
    "    str_name.replace(':', '')\n",
    "    file_name = f\"C:/Users/ayhan/Downloads/assesment_dataset_converter/sinyal/{count}.txt\"\n",
    "    #file_content = array_term_ids[count] + \"\\t\\t\" + \"fold change\\n\"\n",
    "    file_content = array_term_ids[count] + \" rgb hex codes\\n\"\n",
    "    \n",
    "    array_temp = []\n",
    "    for i in array_intersec:\n",
    "        tmp_str = \"\"\n",
    "        array_tmp = []\n",
    "        for j in i:\n",
    "            if j != ',':\n",
    "                tmp_str += j\n",
    "            else:\n",
    "                array_tmp.append(tmp_str.strip())\n",
    "                tmp_str = \"\"\n",
    "        array_tmp.append(tmp_str.strip())  # son değeri de ekle\n",
    "        array_temp = array_tmp\n",
    "    for i in array_temp:\n",
    "        file_content = file_content + i + \" \"\n",
    "        index_of_value = converted_alias_list.index(int(i))\n",
    "        str_tmmmp = fold_change_df[\"fold_change\"][index_of_value]\n",
    "        rgb_hex_code_value = pick_random_color(hex_colors, int(i)) \n",
    "        \n",
    "        #file_content += str(str_tmmmp)\n",
    "        file_content += rgb_hex_code_value\n",
    "        file_content += \"\\n\"\n",
    "    \n",
    "    with open(file_name, \"w\") as file:\n",
    "        file.write(file_content)\n",
    "        \n",
    "    #print(file_content)\n",
    "    file_content = \"\"\n",
    "    \n",
    "    count += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
